{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Fast2","text":"<p>Supersedes \"About us\" section ?</p>"},{"location":"tags/","title":"Tags","text":"<p>Following is a list of relevant tags:</p>"},{"location":"tags/#drools","title":"Drools","text":"<ul> <li>Drools</li> </ul>"},{"location":"tags/#excel","title":"Excel","text":"<ul> <li>Drools</li> </ul>"},{"location":"tags/#java","title":"Java","text":"<ul> <li>Drools</li> <li>Patterns</li> </ul>"},{"location":"tags/#boilerplate","title":"boilerplate","text":"<ul> <li>Contents in Fast2</li> <li>Add data from file name</li> <li>Datasets</li> <li>Documents management</li> <li>Punnets management</li> </ul>"},{"location":"tags/#configuration","title":"configuration","text":"<ul> <li>Patterns</li> </ul>"},{"location":"tags/#content","title":"content","text":"<ul> <li>Contents in Fast2</li> <li>Documents management</li> <li>Punnets management</li> </ul>"},{"location":"tags/#data","title":"data","text":"<ul> <li>Datasets</li> </ul>"},{"location":"tags/#dataset","title":"dataset","text":"<ul> <li>Datasets</li> <li>Documents management</li> </ul>"},{"location":"tags/#document","title":"document","text":"<ul> <li>Documents management</li> </ul>"},{"location":"tags/#java_1","title":"java","text":"<ul> <li>Contents in Fast2</li> <li>Datasets</li> <li>Documents management</li> <li>Punnets management</li> </ul>"},{"location":"tags/#javascript","title":"javascript","text":"<ul> <li>Add data from file name</li> </ul>"},{"location":"tags/#json","title":"json","text":"<ul> <li>Add data from file name</li> </ul>"},{"location":"tags/#punnet","title":"punnet","text":"<ul> <li>Punnets management</li> </ul>"},{"location":"tags/#worker","title":"worker","text":"<ul> <li>Add data from file name</li> </ul>"},{"location":"advanced/","title":"Advanced section","text":"<p>Learn here advanced handling of Fast2 for optimizing your migration process !</p>"},{"location":"advanced/custom-module/","title":"TODO","text":""},{"location":"advanced/drools/","title":"Drools: the Java rules engine","text":"<p>Based on Excel document, \u201cdrools\u201d is a rule engine used to execute code scripts, Java code in our context. Users can define business and/or functional rules as data transformations, mapping, etc. One of the key benefits is its adaptation to any structure and any level of complexity as long as your code respects the punnet structure (quick reminder here if need be \ud83d\ude09). It can easily be shared between your team members for complex project to have concerned people seamlessly involved. Another upside: no development skill is required to build your own rules. Fast2 supports such feature with the ApplyDroolsTask.</p> <p>A sample of Drools spreadsheet can be downloaded to help you getting started.</p> <p> Download drools template</p>","tags":["Drools","Java","Excel"]},{"location":"advanced/drools/#spreadsheet-structure","title":"Spreadsheet structure","text":"<p>The following picture represents a drool sheet as you could find one in an Excel document:</p> <p></p> <p>It\u2019s composed with :</p> <ul> <li>RuleSet</li> </ul> means that the current speadsheet is a decision table <ul> <li>Import</li> </ul> all java classes required, separated by a comma. These are the same packages that would be imported in a regular Java class in order to have the code running properly. <ul> <li>Sequential (optional)</li> </ul> specify here the order in which rules should apply <ul> <li>RuleTable rules</li> </ul> name of the table <ul> <li>NAME column</li> </ul> represents the name of the differents rules <ul> <li>CONDITION column</li> </ul> condition to verify to perform an action <ul> <li>ACTION column</li> </ul> action to perform if all previous conditions have been validated <ul> <li>Variables used are indicated below the column CONDITION (doc : Document)</li> </ul> <p></p>","tags":["Drools","Java","Excel"]},{"location":"advanced/drools/#how-to-read","title":"How to read","text":"<p>Quite simple! A rule is a row read from left to right, as regular code.</p> <p>An empty row is interpreted by the rules engine as the end of the process. Each rule will have to meet particular criteria. There must be at least one condition and one action per rule.</p>","tags":["Drools","Java","Excel"]},{"location":"advanced/drools/#read-a-condition","title":"Read a condition","text":"<ul> <li>A rule can have multiple conditions</li> <li>All conditions must be validated to apply the action of the same row</li> <li>If no value is present in the condition column, the condition is skipped (considered <code>true</code>)</li> </ul> <p>A condition cell will only hold one statement. If several conditions have to be met, they will be in the next columns.</p>","tags":["Drools","Java","Excel"]},{"location":"advanced/drools/#read-an-action","title":"Read an action","text":"<ul> <li>A rule can have multiple actions</li> <li>Actions are performed from left to right</li> <li>Inside a cell, the actions are separated by a semicolon ;</li> </ul> <p>The actions are read just like any code snippet, similarly to a regular script file.</p> <p></p>","tags":["Drools","Java","Excel"]},{"location":"advanced/drools/#parameters","title":"Parameters","text":"<p>There are two different ways to use parameters:</p> <ul> <li>You only need one parameter for your condition or action : <code>$param</code>.</li> <li>Otherwise, separate values by comma, and use <code>$1</code>, <code>$2</code> and so on in you condition/action.</li> </ul> <p></p>","tags":["Drools","Java","Excel"]},{"location":"advanced/drools/#write-a-condition","title":"Write a condition","text":"<p>Conditions, just as in a regular coding snippet, must be performed as a boolean. Actions are executed only if condition is TRUE. It\u2019s highly recommended to use <code>eval(&lt;condition&gt;)</code> or <code>!eval(&lt;condition&gt;)</code> for conditions.</p> <p>Check</p> <p>Just as you would write any condition in your code,</p> <ul> <li>Conditions must not end by a semi-colon (<code>;</code>)</li> <li>Characters allowed : <code>&lt;</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&gt;=</code>, <code>||</code>, <code>&amp;&amp;</code>, \u2026</li> </ul> <p>If you want to perform an action no matter what, do <code>eval($param)</code> with <code>$param = true</code>.</p> <p>If you need the document to have a specific data before making any action, do:</p> <p><code>doc.getDataSet().hasData($param)</code> with <code>$param = yourDataName</code>.</p> <p></p>","tags":["Drools","Java","Excel"]},{"location":"advanced/drools/#action-examples","title":"Action examples","text":"<p>Check</p> <p>You can put any Java code to perform an action, as long as you end each instruction by a semi-colon (<code>;</code>).</p>","tags":["Drools","Java","Excel"]},{"location":"advanced/drools/#add-new-data","title":"Add new data","text":"<p>To add a new data, if you know both the key/name and the value, use the following code :</p> <pre><code>doc.getDataSet().addData(\"&lt;key&gt;\", \"&lt;type&gt;\",\"&lt;value&gt;\");\n</code></pre> <p>In case the value is unknown at the moment or you object is too complex and you might need to add properties to the data object:</p> <pre><code>Data data = doc.getDataSet().addData(\"&lt;key&gt;\", \"&lt;type&gt;\");\ndata.setProperty(\"&lt;key&gt;\",\"&lt;value&gt;\");\n</code></pre> <p>When performing such operation, though, don\u2019t forget to add the proper Fast2 package to manipulate Data type.</p>","tags":["Drools","Java","Excel"]},{"location":"advanced/drools/#add-new-value-to-existing-data","title":"Add new value to existing data","text":"<p>Add one value:</p> <pre><code>doc.getDataSet().getData(\"&lt;data-key&gt;\").addValue(\"$param\");\n</code></pre> <p>Add multiple values to the same data:</p> <pre><code>doc.getDataSet().getData(\"&lt;data-key&gt;\").getValues().addAll(&lt;list-of-values&gt;);\n</code></pre>","tags":["Drools","Java","Excel"]},{"location":"advanced/drools/#stop-a-rule","title":"Stop a rule","text":"<p>You can stop the rule execution at a specific time when an action has been performed. Use <code>drools.halt();</code> in the action section.</p> <p>The next action(s) will not be performed as the rule execution is stopped (useful in case of error management).</p>","tags":["Drools","Java","Excel"]},{"location":"advanced/drools/#good-practices","title":"Good practices","text":"<p>We advise you to create a folder at the root of Fast2 and name it Rules. However, Fast2 will be able to fetch your drools files anywhere as long as the specified path is accessible to the Fast2 server.</p> <p>This path will be fill in the the task ApplyDroolsTask.</p> <p></p>","tags":["Drools","Java","Excel"]},{"location":"advanced/optimization/","title":"TODO","text":"<p>thread management, components configuration (why, how)</p>"},{"location":"advanced/patterns/","title":"Patterns","text":"<p>A pattern is a sequence of instructions, a model, which can be easily recognized by an aware glance. It is strictly under this definition that Fast2 patterns stand.</p> <p>Our migration tool relies on a specific syntax used to dynamically retrieve information from different data-layer of the whole process, whether document, map execution or else.</p>","tags":["configuration","Java"]},{"location":"advanced/patterns/#patterns-what-are-they-anyway","title":"Patterns, what are they anyway ?","text":"<p>In Fast2, several data can be retrieved and accessed dynamically from a dedicated syntax which Fast2 supports for your convenience. This <code>${...}</code> syntax can be used in most of the configuration fields of the tasks composing your migration workflow.</p> <p></p> <p>Using such syntax will comes in handy when you will have to rely on a value whose you only know the name. In other words, retrieving a metadata whose key is <code>doc_mimeType</code> and value is unique for every document, will just be <code>${doc_mimeType}</code>.</p> <p>No need to list all your possible values, Fast2 will resolve this expression by looking first at the document dataset level, then at the punnet level, an later at map/campaign level. Some applications of the latter could be to store the name of the map, or even accessing map- or global-scoped shared ojects for cross-campaign communications.</p>","tags":["configuration","Java"]},{"location":"advanced/patterns/#patterns-in-links","title":"Patterns in links","text":"<p>Although links are designed to offer basic statements for conditional routing, they also digest pattern for higher-complexity conditions. For example, new conditions can be value-dependent: not only you can check whether the document has a given data, but now it is possible to narrow down the eligible documents based on the value itself of this data.</p> <p>Based on SpEL (Spring Expression Language), the syntax of these conditions will sound familiar to anyone who's already coded one day:</p> <p></p> <p>In the same way, you'll now be able to sort documents based on their mime-types, on their structure (does my document has a content ? Is its creation date matching the time range which this campaign is focusing on? ).</p> <p>As mentioned earlier, the list of data which you can evaluate in a condition is the same list as in a task configuration (document properties, punnet properties, map and campaign names).</p>","tags":["configuration","Java"]},{"location":"advanced/patterns/#patterns-subtleties","title":"Patterns subtleties","text":"","tags":["configuration","Java"]},{"location":"advanced/patterns/#properties-with-colon","title":"Properties with colon","text":"<p>As handy as they may sound, patterns do embed specifications due to the particular syntax they are subjected to.</p> <p>The most common issue is when dealing with colon character (<code>:</code>). To prevent running into a SpEL syntax error which would wipe its interest out, the syntax has to be slightly expanded. Where before you were accessing the value with <code>${key}</code>, you now need to write it as follows:</p> <pre><code>${property('prefix:suffix')}\n</code></pre> <p>You can now safely extract data with namespaces, or any special character which may eventually break the SpEL syntax.</p>","tags":["configuration","Java"]},{"location":"advanced/patterns/#propose-default-value","title":"Propose default value","text":"<p>In case the pattern value is not known by Fast2, an empty String is return. However, you might be willing to set a default value to ease the upcoming operations.</p> <p>To do so, use the Elvis ternary operator along the <code>property()</code> function:</p> <pre><code>${property('missingData')?:'defaultValue'}\n</code></pre> <p>From now on, if the 'missingData' is not fount either at the punnet or document level, the value you earlier planned to retrieve will be replaced with the value set as default, although no additional property is created.</p>","tags":["configuration","Java"]},{"location":"advanced/patterns/#access-data-of-fast2-objects","title":"Access data of Fast2 objects","text":"<p>Whether you need subtypes properties for conditional routing or medatada elaboration, Fast2 gives you access to any data stored in the punnet.</p> <p>However targetting object is not always intuitive, so here are the different keywords required to access the Fast2 objects :</p> Keyword Description Examples <code>${CurrentContainer}</code> Access the focused content, to call its properties. This can be quite useful when dealing with multi-contented documents. <code>${CurrentContainer.mimetype}</code> <code>${CurrentAnnotation}</code> Access the annotation of the document. <code>${CurrentAnnotation.annotationId}</code> <code>${punnet}</code> Access the punnet as an object. From there, all datasets and subobjects can be accessed. The accessor is generally used for conditions. <code>${punnetId.toString().startsWith('My')}</code> <code>${documents}</code> The list of the documents stored in the punnet. <code>${documents.size()}</code> <code>${documents.get(0)}</code> <code>${step}</code> The name of the step where the pattern is called. <code>${map}</code> The name of the map which is run during this campaign. Often used for output directory names <code>${map}/my_output_file.csv</code> <code>${campaign}</code> The name of the campaign. Often used for output directory names <code>${map}/${campaign}/my_output_file.csv</code>","tags":["configuration","Java"]},{"location":"advanced/patterns/#using-java-classes","title":"Using Java classes","text":"<p>Pattern can also be used to enrich data, relying on the basic Java classes.</p> <p>Hint</p> <p>The required syntax is <code>T(clazz)</code>.</p> <p>For example, adding an UUID created on-the-fly would just required using the following pattern:</p> <pre><code>${T(java.util.UUID).randomUUID().toString()}\n</code></pre>","tags":["configuration","Java"]},{"location":"advanced/scheduler/","title":"TODO","text":""},{"location":"advanced/shared-objects/","title":"TODO","text":""},{"location":"catalog/","title":"Catalog","text":"<p>All along this documentation concerning the configuration of Fast2 objects (either tasks or tools used within tasks), consider the default value set to <code>false</code> if no default value is mentionned for boolean fields.</p>"},{"location":"components/","title":"Components","text":"<p>blabla bla</p>"},{"location":"components/broker/","title":"The broker","text":"<p>Hint</p> <p>The broker is the workflow orchestrator, in charge of database communication, sending punnets to the worker(s) for them to process the operations.</p>"},{"location":"components/broker/#configure-the-broker","title":"Configure the broker","text":"<p>Depending on the amount of documents you are dealing with, you may want to control max memory usage allowed (Xmx) for broker.</p> <p>By default, only 1GB is allocated for this resource :</p> /config/env.properties<pre><code>...\n# Broker Maximum memory allowed (Xmx)\nBROKER_MAX_MEMORY=1G\n</code></pre> <p>If the campaign are involving a couple of millions of documents, increasing this value to 8GB or 16GB will definitely help increasing the performance rate of the migration.</p> <p></p>"},{"location":"components/broker/#configure-the-ui-port","title":"Configure the UI port","text":"<p>The UI port is also subject to configuration.</p> <p>Fast2 application run on the 1789 port by default. To change this, add or update the parameters below:</p> /config/application.properties<pre><code>...\n# Remote broker port to use by the worker\n# broker.port=1789\nserver.port=1789\n</code></pre>"},{"location":"components/dashboards/","title":"Dashboards","text":"<p>Since Fast2 stores every single byte of migration information into its internal database, using dashboard capabilities for intelligible and functional reports just comes in naturally.</p> <p>Warning</p> <p>Prior to the v2.5, Fast2 was relying on Kibana for data vizualisation. This component has been dropped in favor of OpenSearch dashboards.</p> <p>However the configuration of these tools are very close (if not identical).</p> <p>The dashboard only communicates with the database (as illustrated in the architecture section.</p> <p>All the chart visualizations which can be built up with this add-on and intergrated to the most advanced dashboards, solely serve one purpose: data digestion for tracking progress, by making now possible to follow edge-cases of a handful of documents lost in a week-long non-stopping flood, and building reports out of it.</p>"},{"location":"components/dashboards/#configure-the-dashboards","title":"Configure the dashboards","text":"<p>Fast2 does not embed any dashboard by default. However, you can get the add-on through the same portal you downloaded the Fast2 binaries. Unzip the package at the root of Fast2 installation folder.</p> <p>This hierarchy will make Fast2 automatically start your dashboard.</p>"},{"location":"components/dashboards/#with-or-without","title":"With or without","text":"<p>When Fast2 is booted up, it will by default look for the dashboard folder at the root of the installation folder. If they are unzipped in the right location, they will be started after the broker triggered the database startup.</p> <p>In case no dashboard folder is found, this step will be skipped after a given number of retries (which you can find the in the <code>./config/application.properties</code> file).</p> <p>The dashboards can still be disabled, even if they are available in the root folder of Fast2:</p> v2.4-v2.6+ ./config/application.properties<pre><code>broker.kibana.embedded.enabled=true\n</code></pre> ./config/application.properties<pre><code>broker.dashboards.embedded.enabled=true\n</code></pre>"},{"location":"components/dashboards/#ports","title":"Ports","text":"<p>By default, the dashboards are serve from the port 1791.</p> <p>However they can be accessed on a different port, which you will have to highlight in 2 different places:</p> <ol> <li> <p>This is required to start the add-on on another port:</p> v2.4-v2.6+ ./config/application.properties<pre><code>broker.kibana.embedded.port=8888\n</code></pre> ./config/application.properties<pre><code>broker.dashboards.embedded.port=8888\n</code></pre> </li> <li> <p>Fast2 should know where to send the user for the visualizations:</p> v2.4-v2.6+ ./kibana-X.Y.Z/config/kibana.yml<pre><code>server.port=8888\n</code></pre> ./opensearch-dashboards-X.Y.Z/config/opensearch_dashboards.yml<pre><code>server.port=8888\n</code></pre> </li> </ol>"},{"location":"components/dashboards/#remote-access-to-the-dashboards","title":"Remote access to the dashboards","text":"<p>If this port needs to be exposed and accessible from a different machine, the dashboard configuration must be configured to allow connections from remote users:</p> ./opensearch-dashboards-X.Y.Z/config/opensearch_dashboards.yml<pre><code>server.host: \"0.0.0.0\"\n</code></pre>"},{"location":"components/dashboards/#what-is-the-database-port-is-changed","title":"What is the database port is changed ?","text":"<p>To make sure the dashboards still reach the database, make sure the port is still up-to-date in the dashboards config file :</p> ./opensearch-dashboards-X.Y.Z/config/opensearch_dashboards.yml<pre><code>opensearch.hosts: [\"http://localhost:1790\"]\n</code></pre>"},{"location":"components/dashboards/#advanced-use","title":"Advanced use","text":"<p>Note</p> <p>If the metadata you are looking for is not available and cannot be found in the dropdown options, refresh the <code>f2_*</code> index (which can be manually triggered from the list of saved objects).</p> <p>The dashboards add-on provided with Fast2 is the go-to tool for migration report, project advancement insights, and deeper data analysis.</p> <p>However data manipulations in this tool are not always intuitive nor straight forward, although they do open new dimensions regarding in-depth studies by the compound aggregation, data conversion and other operations now at the tips of your fingers.</p> <p>Several use-cases can be envisioned, we will only relate here the data conversion steps to go through given the widespread necessity of such a basic task.</p>"},{"location":"components/dashboards/#imports-objects-into-the-dashboards-feature","title":"Imports objects into the dashboards feature","text":"<p>This section will guide you through the import process of resources (such as indices, visualization as <code>.ndjson</code> files and others).</p> <p>This resource can be imported into your dashboard add-on from the right-side menu &gt; \"Stack management\" &gt; \"Saved objects\" &gt; \"Import\"</p> <p>as shown on the screen-capture below :</p> <p></p>"},{"location":"components/dashboards/#resource-1-exception-table","title":"Resource #1 : Exception table","text":"<p>Info</p> <p>This resource has been generated from OpenSearch dashboards but can be imported into either Kibana and OpenSearch dashboards.</p> <p>For a list of exceptions, since the error messages and steps in error are tracked in the database but not attached to the punnet itself, it is possible to rely on the dashboards add-on to generate a table of exceptions, accross multiple campaigns. With all the punnet details required for both investigations and error resolution, but for delta migrations afterwards, this table can be exported in CSV for externalisation of this asset.</p> <p>Here is an example of a table gathering :</p> <ul> <li>Campaign: campaign where the exception is thrown</li> <li>PunnetId: ID of the punnet</li> <li>documentId: ID of the document (useful when the punnets store several documents)</li> <li>Document name</li> <li>Step where the exception occured</li> <li>Exception class</li> <li>Exception message</li> <li>Content URL (if any)</li> <li>Count of the number of documents in the punnet</li> </ul> <p></p> <p>To get started with this visualisation, or to add it to your existing dashboard, click down below :</p> <p> Download this resource</p> <p>This resources can be imported as explained previously.  </p>"},{"location":"components/dashboards/#resource-2-campaign-success-ratio","title":"Resource #2 : Campaign success ratio","text":"<p>Info</p> <p>This resource has been generated from OpenSearch dashboards but can be imported into either Kibana and OpenSearch dashboards.</p> <p>For a graph visualization of the success ratio per map/campaign, the following resource can be imported for a per-day granularity of the results, where exceptions are summed up (no task differenciation), for comparison with the successfully processed documents within this same campaign.</p> <p></p> <p>To get started with this visualisation, or to add it to your existing dashboard, click down below :</p> <p> Download this visualization</p> <p>This resources can be imported as explained previously.  </p>"},{"location":"components/dashboards/#resource-3-processing-speed-per-task","title":"Resource #3 : Processing speed per task","text":"<p>Info</p> <p>This resource has been generated from OpenSearch dashboards but can be imported into either Kibana and OpenSearch dashboards.</p> <p>For a graph visualization of the success ratio per map/campaign, the following resource can be imported for a per-day granularity of the results, where exceptions are summed up (no task differenciation), for comparison with the successfully processed documents within this same campaign.</p> <p></p> <p>To get started with this visualisation, or to add it to your existing dashboard, click down below :</p> <p> Download this visualization</p> <p>This resources can be imported as explained previously.  </p>"},{"location":"components/dashboards/#advanced-filtering-capabilities","title":"Advanced filtering capabilities","text":"<p>Since the visualisations can pull out vast amounts of data from the database, most of the results might need to be narrowed down using the filter function :</p> <p></p> <p>Head out to the matching documentation (Kibana or OpenSearch dashboards) for basic rules and help on how to build such filter.</p> <p>We will here just focus on one main filter, which would help to only get the relevant data for either a ratio or datatable of success or failure along the migration.</p> <p>Our need is to only the the documents/punnets, whose status are <code>ProcessedException</code> (to gather all failed documents, no matter the task where the exception got thrown) or the documents/punnets being both <code>ProcessedOK</code> from the injection task (which will be called here: Last task).</p> <p>In short, we only want to select :</p> <ul> <li>the OK's of the injector, which induces the success of the migration for this document</li> <li>the KO's of all the tasks</li> </ul> <p>Code-wise, since our expression would looks like this :</p> <pre><code>status == KO || (status == OK &amp;&amp; step == \"Last task\");\n</code></pre> <p>Since</p> <ul> <li><code>||</code> is should</li> <li><code>&amp;&amp;</code> is must</li> </ul> <p>the final syntax is (for DSL -- Dashboards Query Language -- or KQL -- Kibana query language) :</p> <pre><code>{\n\"query\": {\n\"bool\": {\n\"should\": [\n{\n\"term\": { \"status.keyword\": \"ProcessedException\" }\n},\n{\n\"bool\": {\n\"must\": [\n{ \"term\": { \"status.keyword\": \"ProcessedOK\" } },\n{ \"term\": { \"stepName.keyword\": \"Last task\" } }\n]\n}\n}\n]\n}\n}\n}\n</code></pre> <p>This code is to be used in the filter function, as advanced filter (instead of the default fields-prepared option).</p> <p></p> <p></p> <p>The dashboards add-on provided with Fast2 is the go-to tool for migration report, project advancement insights, and deeper data analysis.</p> <p>However data manipulations in this tool are not always intuitive nor straight forward, although they do open new dimensions regarding in-depth studies by the compound aggregation, data conversion and other operations now at the tips of your fingers.</p> <p>Several use-cases can be envisioned, we will only relate here the data conversion steps to go through given the widespread necessity of such a basic task.</p>"},{"location":"components/dashboards/#datatype-conversions","title":"Datatype conversions","text":"<p>Let\u2019s consider a metadata processed by Fast2 as a String instead of a float. One frequent use-case could be reporting the sum of all content size processed during a campaign. Adding up String values never ended up well so far, Kibana will have to parse these values beforehand, to have the user access the newly created value with the correct type.</p> <p>We will base our example on the following punnet structure:</p> punnet.xml<pre><code>&lt;?xml version='1.0' encoding='UTF-8'?&gt;\n&lt;ns:punnet xmlns:ns=\"http://www.arondor.com/xml/document\" punnetId=\"FileNetSource#page_0#pageIndex_0\"&gt;\n&lt;ns:documentset&gt;\n&lt;ns:document documentId=\"{1B62F7C4-8E75-4D99-B84C-0AAD14B13A4E}\"&gt;\n&lt;ns:contentset&gt;\n&lt;ns:content mimeType=\"application/pdf\"&gt;\n&lt;ns:url&gt;path/to/file/content&lt;/ns:url&gt;\n&lt;/ns:content&gt;\n&lt;/ns:contentset&gt;\n&lt;ns:dataset&gt;\n&lt;ns:data name=\"MimeType\" type=\"String\"&gt;\n&lt;ns:value&gt;application/pdf&lt;/ns:value&gt;\n&lt;/ns:data&gt;\n&lt;ns:data name=\"ContentSize\" type=\"String\"&gt;\n&lt;ns:value&gt;43315.0&lt;/ns:value&gt;\n&lt;/ns:data&gt;\n&lt;ns:data name=\"name\" type=\"String\"&gt;\n&lt;ns:value&gt;file_name&lt;/ns:value&gt;\n&lt;/ns:data&gt;\n...\n            &lt;/ns:dataset&gt;\n&lt;ns:folderset /&gt;\n&lt;ns:annotationset /&gt;\n&lt;/ns:document&gt;\n&lt;/ns:documentset&gt;\n&lt;folderSet /&gt;\n&lt;/ns:punnet&gt;\n</code></pre> <p>Here, the data type of the ContentSize property is <code>String</code>, as the type attribute states. Our job will be to parse this <code>String</code> value to <code>Float</code>, since we have a decimal.</p> <p>This operation happens in 2 steps:</p> <ol> <li>Making the original field (with the wrong type) accessible from a script,</li> <li>Writing the correct parsing script.</li> </ol> <p></p>"},{"location":"components/dashboards/#step-1-making-the-field-accessible","title":"Step 1: Making the field accessible","text":"<p>Open the dashboards retrieving data from the database where Fast2 is sending the data.</p> <p>Access the Dev Tools of the dashboards to execute the query, which has to be built a specific way in order to change the mapping of the field to convert.</p> <p>The index prefix of Fast2 data is <code>f2_</code>, which is why the request has to specify the index to apply the <code>PUT</code> operation on.</p> <p>Next you have to write in the request body the whole way to the property to convert, first by accessing the punnet, then the properties of the punnet where the documents are stored, then the properties of the documents object where the metadata are stored, and so on.</p> <p>Finally specify the current type (<code>text</code> in our case), with <code>fielddata:true</code>.</p> <p>The final query will look like such:</p> <pre><code>PUT f2_*/_mapping\n{\n\"properties\": {\n\"punnet\": {\n\"properties\": {\n\"documents\": {\n\"properties\": {\n\"data\": {\n\"properties\": {\n\"ContentSize\": {\n\"type\": \"text\",\n                  \"fielddata\": true\n}\n}\n}\n}\n}\n}\n}\n}\n}\n</code></pre> <p>The query is successfully executed once the <code>acknowledged:true</code> message is returned.</p> <p></p>"},{"location":"components/dashboards/#step-2-the-parsing-script","title":"Step 2: The parsing script","text":"<p>Head now to the Dashboads Management section, choose \u2018Index patterns\u2019, and select the one related to Fast2 (<code>f2_*</code>).</p> <p>Hint</p> <p>You may require to refresh the <code>f2_*</code> index, to make sure all the latest properties are fetched from the database.</p> <p>Click on the \"Scripted field\" tab to create a new one using the data property you just made accessible.</p> <p>Enter a relevant name (ex/ ContentSize-float), select the new type of this field (<code>number</code>, for our example), and write the following script:</p> <pre><code>if (doc.containsKey(\"punnet.documents.data.ContentSize\"))\nreturn Float.parseFloat(doc[\"punnet.documents.data.ContentSize\"].value);\nelse return -1;\n</code></pre> <p>Save this new field, and create a new visualization displaying a sum of this data per campaign (as this was our initial challenge).</p> <p>In the sum section, our field is now reachable for any chart requiring numerical data.</p> <p>The <code>String</code> data is now accessible as a numerical chart field !! </p> <p>And that\u2019s it !</p> <p>More use cases can be addressed by scripts with higher level of complexity, to create new data, digest or sort data, you name it.</p> <p>This new dimension of data analysis via Kibana opens up way more possibilities, while increasing the precision of data aggregation to bring the best answers to the project management team.</p> <p></p>"},{"location":"components/dashboards/#faq","title":"FAQ","text":""},{"location":"components/dashboards/#run-fast2-without-dashboards","title":"Run Fast2 without dashboards","text":"<p>It is possible to run Fast2 without the dashboards, as this add-on is just reading data stored in the internal database, to serve them as graphical vizualisation. This</p> <p>By no mean this add-on is necessary for the migration.</p>"},{"location":"components/dashboards/#access-dashboard-when-not-migrating","title":"Access dashboard when not migrating","text":"<p>Visualizations can be reached via 2 different ways, depending on your needs:</p> <ul> <li>Either from the UI of Fast2,</li> <li>Or directly from the browser via the declared port (1791 or other).</li> </ul> <p>The latter option gives access to the tool even when Fast2 is not running.</p> <p>Since the dashboards fetch data directly from the database, it is not possible to populate the visualizations with migration data if the database is not running as well.</p>"},{"location":"components/dashboards/#dashboards-do-not-reach-the-database","title":"Dashboards do not reach the database","text":"<p>Make sure the database port has been correctly configured in the YAML file of the dashboards. Head to the port section for more details.</p>"},{"location":"components/dashboards/#could-not-ping-kibana-on-port-1791","title":"Could not ping Kibana on port 1791","text":"<p>Make sure declaring your port in the 2 expected places. Head to the port section for more details.</p>"},{"location":"components/database/","title":"Internal database","text":"<p>Warning</p> <p>Prior to the v2.5, Fast2 was relying on an Elasticsearch database. This component has been dropped in favor of OpenSearch.</p> <p>However the configuration of these two databases are very close (if not identical).</p> <p>Every object passing through Fast2 is stored into an internal database. Whether carried by the document or the punnet, all metadata are recorded in the warehouse. The major benefit of such architecture is the opportunity to check whether everything is going well by making counters about documents/data processed during your migrations.</p> <p>In addition, we can easily rollback or resume operations in case of server crash. Nothing will be lost as Fast2 will precisely know where it all stopped.</p> <p>There is the logic behind real-time backups in ES.</p>"},{"location":"components/database/#indexes","title":"Indexes","text":"<p>Each Elasticsearch index referenced by Fast2 will be registered with a <code>f2_</code> prefix. An index is always written in lower case even if the campaign name in Fast2 contains characters in upper cases.</p> <p>For example, the campaign <code>MyCampaign_Try10</code> will be stored in the index f2_mycampaign_try15.</p> <p>During the step of broker intantiation at Fast2 startup, some indices are automatically created:</p> Index key Description f2_campaigns List of existing campaigns with processing dates and status f2_campaigns_sources Links between campaigns and workers having performed this campaign f2_queue_settings Reference information about source and task threads f2_jobs_settings Gather the configuration of save jobs f2_jobs_info Information about jobs past execution details <p>For each new campaign of Fast2, an index will be created: if you decided to run a new campaign named <code>EcmInjection</code>, the new index will be <code>f2_ecminjection_try1</code>.</p>"},{"location":"components/database/#configuration","title":"Configuration","text":""},{"location":"components/database/#with-or-without","title":"With or without","text":"<p>For an optimal migration setup, this third-party software can be easily configured at different levels to match you needs at most ! If required, it can even be disabled at will.</p> v2.4-v2.5+ ./config/applications.properties<pre><code>broker.elasticsearch.embedded.enabled=true\n</code></pre> ./config/applications.properties<pre><code>broker.opensearch.embedded.enabled=true\n</code></pre>"},{"location":"components/database/#port","title":"Port","text":"<p>By default, Fast2 sends it data via the Elasticsearch API made available on port <code>1790</code>.</p> <p>However, in the case where this port is already used by either another Fast2 instance or any other process, the port number can be changed from the configuration files.</p> <p>Since Elasticsearch has to be reach from both Fast2 broker and Kibana module \u2014 if the latter is enabled. If any Elasticsearch port had to be reconfigured to match specific needs, there is exactly 3 places where to mention this change:</p> v2.4-v2.5+ File Specification ./config/application.properties <code>opensearch.port=1790</code> ./elasticsearch-X.Y.Z/config/elasticsearch.yml <code>http.port: &lt;es-port&gt;</code> ./kibana-X.Y.Z/config/kibana.yml <code>elasticsearch.hosts: [\"http://localhost:&lt;es-port&gt;\"]</code> File Specification ./config/application.properties <code>opensearch.port=1790</code> ./opensearch-X.Y.Z/config/opensearch.yml <code>http.port: &lt;es-port&gt;</code> ./opensearch-dashboards-X.Y.Z/config/opensearch_dashboards.yml <code>opensearch.hosts: [\"http://localhost:&lt;es-port&gt;\"]</code> <p>If the dashboard component is installed, the database port also needs to be updated on this front as the dashboard needs to access the DB in order to read the data :</p> v2.4-v2.6+ ./kibana-X.Y.Z/config/kibana.yml<pre><code>elasticsearch.hosts: [\"http://&lt;DB-server:DB-port&gt;\"]\n</code></pre> ./opensearch-dashboards-X.Y.Z/config/opensearch_dashboards.yml<pre><code>opensearch.hosts: [\"http://&lt;DB-server:DB-port&gt;\"]\n</code></pre> <p></p>"},{"location":"components/database/#memory","title":"Memory","text":"<p>The more documents, the more data. The more data, the more Elasticsearch will need resources to digest, store, process data and respond to the broker.</p> <p>Head out to the <code>./opensearch-X.X.X/config/jvm.options</code> file.</p> <p>The configuration required are the following:</p> Configuration Purpose <code>-Xms4g</code> This setting will allocate 4GB of RAM to the Elasticsearch JVM heap, directly on startup. <code>-Xmx8g</code> Here, you specify the maximum memory which can be used, if available, by the database. <code>\u2212XX:MaxDirectMemorySize=4g</code> A Java process/service doesn't use only the amount of memory defined for the JVM Heap but it will make use native (off-heap) memory. For a JVM Max Heap size of 4GB, in recent versions of Elasticsearch is going to limit the <code>XX:MaxDirectMemorySize</code> to 50% of the JVM Max Heap size (2GB in our case) for direct memory. <p>For further comprehension of these parameters, check out the Elasticsearch official documentation on the topic or OpenSearch official documenation. Upgrading the metrics will prevent <code>java.lang.OutOfMemoryError</code> to pop up during heavy migration executions.</p> <p></p>"},{"location":"components/database/#remote-access-to-the-database","title":"Remote access to the database","text":"<p>The next operations need to happen when the database is shut down. To make sure of that, the <code>jcmd</code> command might be of great help.</p> <p>Warning<p>The database port needs to be opened from the Fast2 server, and accessible by your remote machine.</p> </p> <ol> <li> <p>To check the database port is accessible from your machine, run the following command (from your work station):</p> <pre><code>curl &lt;fast2.server&gt;:&lt;database-port&gt;\n</code></pre> </li> <li> <p>Head out to the database configuration YAML file <code>opensearch.yml</code> and add the following lines:</p> <pre><code>network.host: 0.0.0.0\nnode.name: node-1\ncluster.initial_master_nodes: node-1\n</code></pre> </li> <li> <p>If Fast2 is installed on a Linux server, you may also need to increase the memory usage for your cluster (on the server where the database is running), as stipulated in the Official OpenSearch documentation.</p> /etc/sysctl.conf<pre><code># for remote access to Fast2 embedded database\nvm.max_map_count=262144\n</code></pre> </li> <li> <p>Save the file, and run the following command to refresh your server configuration :</p> <pre><code>sudo sysctl -p\n</code></pre> </li> <li> <p>Restart the broker (which will induce the bootup of the database), and go check from your machine the access to the Fast2 database.</p> <p>The webpage (at the same URL we <code>curl</code>-ed in the 1st step) should display something like so :</p> <p></p> </li> </ol> <p></p> <p></p>"},{"location":"components/database/#troubleshooting","title":"Troubleshooting","text":"<p>In reason of the tight commmunication between the broker and the database, chances are you will soon be reported 500 server error generated by unsuccessful exchanges of the two entities.</p>"},{"location":"components/database/#server-error-500-when-starting-a-campaign","title":"Server error 500 when starting a campaign","text":"<p>After running quite a bunch of campaigns, you might end up not being able to start anymore of them due to the limit of shards of the Elasticsearch database (for more in-depth details about the shards, checkout the Official Elasticsearch documentation).</p> <p>The symptom of this limitations comes as a regular 500 server error toast in the UI, but is is by checking the logs/broker.log file that its raw nature is exposed:</p> <pre><code>17:24:45.017 [http-nio-1789-exec-17] ERROR org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/]:175 - Exception while dispatching incoming RPC call com.google.gwt.user.server.rpc.UnexpectedException: Service method 'public abstract com.fast2.model.taskflow.Campaign com.fast2.hmi.gwt.client.service.GWTCampaignManager.startProcessing(\n    com.fast2.model.taskflow.Campaign,com.fast2.model.taskflow.design.TaskFlowMapRef,boolean)' threw an unexpected exception:\n    java.lang.RuntimeException: Caught exception Elasticsearch exception [type=validation_exception, reason=Validation Failed: 1: this action would add [2] total shards, but this cluster currently has [1000]/[1000] maximum shards open;]\n    ...\nCaused by: org.elasticsearch.ElasticsearchStatusException: Elasticsearch exception [type=validation_exception, reason=Validation Failed: 1: this action would add [2] total shards, but this cluster currently has [1000]/[1000] maximum shards open;]\n</code></pre> <p>As mentioned in the Elasticsearch technicalities, Fast2 records data under indices prefixed with <code>f2_</code>. Thus it implies to begin each index to delete with this prefix.</p> <p>Although a drastic cleanu-up induced by a <code>curl -X DELETE -i \"http://&lt;elasticsearch-server&gt;:&lt;elasticsearch-port&gt;/f2_*</code> would resolve our issue, you might be interested in keeping some campaigns or indices. As any curl query allows, exceptions can be added to the deletion operation to prevent them from being removed of the backup database. The syntax goes as follows:</p> <pre><code>curl -X DELETE -i \"http://&lt;elasticsearch-server&gt;:&lt;elasticsearch-port&gt;/f2_*,-f2_campaigns,-f2_campaigns_sources[,-f2_&lt;campaign-name&gt;]\"\n</code></pre> <p>  Let us now study this query:  </p> Section Purpose http://server:port/f2_* Here we ask to aim at all indices starting with the <code>f2_</code> prefix. This will prevent the deletion of additional indices which could be related to parallel work on the same Elasticsearch instance, such as Kibana reports, charts or data analysis. <code>-f2_campaigns,-f2_campaigns_sources</code> These 2 indices are needed if you decide to keep any other campaign. The <code>-</code> sign declares them as exception from the delete action. <code>[,-f2_&lt;campaign-name&gt;]</code> Then you can list all the campaigns you are willing to preserve, <ul><li>without space,</li><li>separated by commas (<code>,</code>)</li><li>mentioning the <code>-</code> exclusion character</li></ul>Do not forget to begin each name with the indice prefix. <p></p> <p>Wildcards are supported, therefore an exception written <code>-f2_mycampaign*</code> will protect all the campaign with this myCampaign radical (ex/ myCampaign_Try1, myCampaign_Try2...).</p>"},{"location":"components/database/#impossible-to-get-results-from-explorer-place","title":"Impossible to get results from Explorer place","text":"<p>Documents concerned by a migration can quickly add up, especially on cumulative campaigns. When heading to the Explorer to check punnets results, you may end up with too much information to retrieve from the database. An \"error 500\" message will then pop up, and if yu head out to the broker.log, the following stack will be found:</p> <pre><code>Suppressed: org.elasticsearch.client.ResponseException: method [POST], host [http://localhost:1790], URI [/f2_campaign_try1/_search?pre_filter_shard_size=...], status line [HTTP/1.1 503 Service Unavailable]\n{\"error\":{\"root_cause\":[{\"type\":\"too_many_buckets_exception\",\"reason\":\"Trying to create too many buckets. Must be less than or equal to: [10000] but was [10001]. This limit can be set by changing the [search.max_buckets] cluster level setting.\",\"max_buckets\":10000}],\"type\":\"search_phase_execution_exception\",\"reason\":\"all shards failed\",\"phase\":\"query\",\"grouped\":true,\"failed_shards\":[{\"shard\":0,\"index\":\"f2_stg-100k_try1\",\"node\":\"4XM6VD2wTfeOSpr2brIs7g\",\"reason\":{\"type\":\"too_many_buckets_exception\",\"reason\":\"Trying to create too many buckets. Must be less than or equal to: [10000] but was [10001]. This limit can be set by changing the [search.max_buckets] cluster level setting.\",\"max_buckets\":10000}}]},\"status\":503}\n</code></pre> <p>To fix this, you need to stop Fast2 (and the database), and add the following line:</p> ./elasticsearch-X.Y.Z/config/elasticsearch.yml<pre><code>search.max_buckets: 1000000\n</code></pre>"},{"location":"components/database/#lets-quickly-wrap-up-here","title":"Let's quickly wrap up, here","text":"<p>This integrated database guarantees data persistence to Fast2. If required, an Elasticsearch database can be shared among several Fast2 servers.</p> <p> Allocated resources should be increased in order to resist charge of production environments.</p>"},{"location":"components/worker/","title":"The worker","text":"<p>Hint</p> <p>The Worker is the punnet processor, applying the changes onto the punnet, according to how the tasks have been configured by the user.</p> <p>The workers! Corner stones of Fast2, these guys can litterally add up and constitute a real digitized hive working to migrate your documents, your contents, your rules, your metadata, all synchronously, exactly where you expect them (or asked them to be), never stepping on each other. No migration project could be overcome if it was not by them!</p> <p>If they role can seem quite important, they are paradoxically as easy and straight forward to get up and running. Just the right files to gather, as mentioned here, and a new worker just enrolled!</p> <p>One of the major aspects of a promising migration project is what all project managers will ask you to vouch for: performance metrics. Let\u2019s suppose you need to migrate documents from a source system into a second one, the latter having a much higher input flow tolerance. No need for empirical statistics to know that the old ECM will be the bottleneck. An architecture similar to a hybrid deployment variant (topic we presented here) could easily be envisioned. But let\u2019s complicate things a little bit here: in-between the extraction and the injection phase, the metadata have to be updated, with new date formatting and heavy mapping of document related properties. Can still a hybrid-like architecture save you now ?</p>"},{"location":"components/worker/#configure-the-workers","title":"Configure the worker(s)","text":"<p>The required files for the worker to run properly are the following:</p> Item Purpose  config/* Configuration files, broker endpoint etc  worker-libs/* All libraries and dependencies for tasks executions  fast2-worker-package-X.Y.Z.jar Worker main unit  startup-worker.bat Binary file for Windows  startup-worker.sh Binary file for Linux"},{"location":"components/worker/#memory-allocation","title":"Memory allocation","text":"<p>Depending on the amount of documents and the number of tasks you are dealing with, you may want to control max memory usage allowed (<code>Xmx</code>) for worker.</p> <p>The default setting is 1GB for this resource:</p> ./config/env.properties<pre><code>...\n# Worker Maximum memory allowed (Xmx)\nWORKER_MAX_MEMORY=1G\n</code></pre> <p>Keep in mind that this property is designed for workers started from the binary <code>start-worker.sh|.bat</code>. If you intend to target the embedded worker, go to <code>./config/application.properties</code> instead:</p> <pre><code>...\n# Broker embedded worker max memory\nbroker.embeddedworker.max.memory=1G\n</code></pre>"},{"location":"components/worker/#queues-management","title":"Queues management","text":"<p>Queues have to be declared to the workers for them to process the punnets stored in these sames queues.</p> <p>The queues names will also be declared in the tasks configuration panel, so the only worker in charge of executing a task with a defined queue will be the worker whose queue regex matched this very queue.</p> <p>In order to have specific worker tied to particular queues, the configuration needs to be updated here:</p> ./config/application.properties<pre><code># Worker queue regex filter\n# worker.queue.regex=.*\n</code></pre>"},{"location":"components/worker/#advanced-use","title":"Advanced use","text":"<p>One of the major aspects of a promising migration project is what all project managers will ask you to vouch for: performance metrics. Let\u2019s suppose you need to migrate documents from a source system into a second one, the latter having a much higher input flow tolerance. No need for empirical statistics to know that the old ECM will be the bottleneck. An architecture similar to a hybrid deployment variant (topic we presented here) could easily be envisioned. But let\u2019s complicate things a little bit here: in-between the extraction and the injection phase, the metadata have to be updated, with new date formatting and heavy mapping of document related properties. Can still a hybrid-like architecture save you now ?</p>"},{"location":"components/worker/#several-workers","title":"Several workers","text":""},{"location":"components/worker/#context","title":"Context","text":"<p>One of the major aspects of a promising migration project is what all project managers will ask you to vouch for: performance metrics. Let\u2019s suppose you need to migrate documents from a source system into a second one, the latter having a much higher input flow tolerance. No need for empirical statistics to know that the old ECM will be the bottleneck. An architecture similar to a hybrid deployment variant (topic we presented here) could easily be envisioned. But let\u2019s complicate things a little bit here: in-between the extraction and the injection phase, the metadata have to be updated, with new date formatting and heavy mapping of document related properties. Can still a hybrid-like architecture save you now ?</p>"},{"location":"components/worker/#how-to-set-up","title":"How to set up","text":"<p>Checkout in the official documentation the required Fast2 files and folders to set up a new worker. Leave a copy of the required files and folder on the machine hosting the source environment. This worker -- let\u2019s label it as worker-S for \u201csource\u201d -- will be assigned to the extraction part. As indicated in the installation section, starting Fast2 will launch an embedded worker, assigned by default to all tasks composing the migration workflow. This worker here will be our worker-D (for \u201cDestination\u201d, or \u201cDefault\u201d).</p> <p>Plug the worker-S onto the Fast2 broker (yes, the workers -- as illustrated here -- manifest themselves to the broker, and not the other way around) : to do so, open the <code>config/application.properties</code> of the worker-S :</p> v2.4-v2.5+ ./config/application.properties<pre><code># Fast2 2.1.0 configuration\n# Remote broker host to use by the worker\nbroker.host=localhost\n# Remote broker port to use by the worker\n# broker.port=1789\n...\n</code></pre> ./config/application.properties<pre><code># Fast2 2.8.0 configuration\n# Remote broker url to use by the worker\nbroker.url=http://localhost:1789/broker\n# Port exposed by Broker\nserver.port=1789\n# Context path used by Broker\nserver.servlet.context-path=/\n...\n</code></pre> <p>Update the name (or IP address) of the machine where Fast2 is running (<code>broker.host</code>), and the name of the queue which the worker will be assigned to (ex/ \u201cextraction\u201d).</p> <ol> <li>Start now the Fast2 server (documentation here) to have it up and running alongside the worker-D. This latter will be assigned to both the mapping of the metadata and the injection of the documents in the destination environment.</li> <li>The start the worker-S (documentation here).</li> </ol> <p></p> <p>Open your browser to reach the Fast2 UI, and the build up your migration workflow. For the sake of this example, 3 tasks only will suit our needs of extraction, metadata transformation and load.</p> <p>3 tasks, 3 queues, 2 workers: lock and load !!</p> <p>The extraction task will be linked to the same queue we mentioned in the <code>config/application.properties</code> of the lone worker (ex/ source-queue).</p> <p>No need to set a queue for the last task, as it will be handled by default by the last worker started with the Fast2 server.</p> <p></p> <p>For this worker, the <code>config/application.properties</code> will have the queue details set as follows:</p> <pre><code>...\n# Worker queue regex filter\nworker.queue.regex=source-queue\n</code></pre> <p></p> <p>For this worker, the <code>config/application.properties</code> will have the queue details set as follows:</p> <pre><code>...\n# Worker queue regex filter\nworker.queue.regex=metadata-queue,default\n</code></pre>"},{"location":"components/worker/#limits","title":"Limits","text":"<p>Just like any architectural decisions, such model comes with is drawbacks and benefits. If the benefits can sound quite obvious given past explanations, the downsides are worth mentioning. We will shortly discuss here aout the two the most current:</p> <ul> <li>Resource sharing: the more workers you\u2019ll start on the same machine, the less they\u2019ll have individually available resources.</li> <li>Connections and sessions: duplication of workers induces duplications of server calls, therefore opened sessions.</li> </ul>"},{"location":"components/worker/#resource-sharing","title":"Resource sharing","text":"<p>Let\u2019s consider a migration server with 8GB of RAM (which is a pretty good start, don\u2019t get me wrong): with a running database in the background \u2014 the embedded Elasticsearch instance which Fast2 relies on in terms of traceability \u2014 needing roughly 3GB, the operating system using 3GB as well, you\u2019ll end up with only 2GB for your worker to open around 100 documents per second and performing content conversion, metadata transformation etc. Needless to say, adding a second worker won\u2019t help you much here !</p> <p>Increase the threads amount (which you can do in the server configuration, straight from the Fast2 UI) of the queues on which you worker will get the punnets to process will surely be the go-to way for increasing your current performances.</p> <p>The most recommended scale-up here would be to start another worker on a different machine, using totally independent physical resources and combining them to the ones already solicited by the Fast2 server.</p>"},{"location":"components/worker/#connections-and-sessions-maxout","title":"Connections and sessions maxout","text":"<p>A second non-negligible aspect is the number of connections and sessions opened by the workers to communicate with both the source and destination environments. Adding worker will consequently increase these numbers, especially if several threads have been allocated to their processing queues.</p> <p> </p>"},{"location":"components/worker/#and-what-about","title":"And what about...","text":""},{"location":"components/worker/#several-workers-on-the-same-machine","title":"Several workers on the same machine?","text":"<p>One easy application of multi-worker architecture could be the need of having several source system to extract documents from, via dedicated maps for each. Booting up several workers associated with the right task queues will provide sufficent segmentation to have your migration happen simultaneously.</p> <p>Sessions conflicts can be prevented as well but such choices of architecture. At the end of the day, only one Fast2 server will have been managing all your different workflows, all you data will be stored in the very same Elasticsearch database, all with significantly better performance rates!</p>"},{"location":"components/worker/#several-workers-on-the-same-queue","title":"Several workers on the same queue?","text":"<p>In case of seeking for more physical resources for your Fast2 server which, let\u2019s say, is not a scalable machine, you could envision to \u201cplug\u201d a second server to the first one: start another worker on the second machine, and have it aim to the initial Fast2 server where the broker is running. This separate worker will be able to process any task of your workflow, any queue as well, just like the embedded one.</p> <p>However there would be absolutely no point in starting another worker assigned to the same queues as the embedded one on the Fast2 server. That won\u2019t positively affect you performance rates. If that was you goal before scrolling this page, the secret relies in adding more threads to your queues (as mentioned earlier)!</p>"},{"location":"cookbooks/","title":"Fast2 cookbooks","text":"<p>Hint</p> <p>A cookbook in the programming context is collection of tiny programs that each demonstrate a particular programming concept. The Cookbook Method is the process of learning a programming language by building up a repository of small programs that implement specific programming concepts.</p> <p>Here are some examples of short use-cases with Fast2, covering APIs management which might be required for configuring some of the tasks.</p> <p>This section will particularly be insightful for custom module development.</p> <p>You will find here boilerplate code snippets, as well as real-life examples of task configuration for challenges you might take up, one day.</p>"},{"location":"cookbooks/content_basics/","title":"Contents in Fast2","text":"<p>In Fast2, contents are objects embedding the \"file\" (= binary format) of the document. They can be found within either documents themselves or annotations, and can be accessed through different ways. Contents usually hold a mime-type property, alongside any other property closely related to the content itself.</p> <p>Contents are often referred as ContentContainers.</p>","tags":["boilerplate","content","java"]},{"location":"cookbooks/content_basics/#how-to-create-a-content","title":"How to create a content","text":"<p>This section relates of how to add a content from the code.</p> <p>If you wish to add a content (or delete it), head out to the AlterDocumentContent task.</p> <pre><code>// From an URL or a path\nContentContainer myContent = task.getManager()\n.getPunnetContentFactory()\n.createContent(myDocument, myUrl);\n// From an inputstream\nContentContainer myContent = task.getManager()\n.getPunnetContentFactory()\n.createContent(myPunnet, myDocument, myInputStream);\n// From a byte array\nContentContainer myContent = task.getManager()\n.getPunnetContentFactory()\n.createContent(myPunnet, myDocument, myByteArray);\n</code></pre>","tags":["boilerplate","content","java"]},{"location":"cookbooks/content_basics/#how-to-access-a-content","title":"How to access a content","text":"<p>When digging into the structure of a punnet from the Explore place, you'll come across an URL pointing to the location of the binary file.</p> <p>However there is quite a few ways of accessing a given content:</p> <pre><code>// As java file\nFile myFile = task.getManager()\n.getPunnetContentFactory()\n.getContentAsFile(myContent);\n// As byte array\nbyte[] myBytes = task.getManager()\n.getPunnetContentFactory()\n.getContentAsByteArray(myContent);\n// As URL\nURL myURL = task.getManager()\n.getPunnetContentFactory()\n.getContentAsUrl(myContent);\n// As RandomAccessInterface\nRandomAccessInterface myRAI = task.getManager()\n.getPunnetContentFactory()\n.getContentAsRandomAccessInterface(myContent);\n</code></pre>","tags":["boilerplate","content","java"]},{"location":"cookbooks/content_basics/#mime-type","title":"Mime-type","text":"<p>The content mime-type is a property usually added by the MimetypeFinder task. However you could be willing to force it, which can be done like so:</p> <pre><code>myContent.setMimeType(\"the right mime-type\");\n</code></pre> <p>This is basically what the MimetypeFinder will do once the mime-type resolved from the content format.</p> <p>To access this value, a regular java getter will do:</p> <pre><code>String myMimetype = myContent.getMimeType();\n</code></pre>","tags":["boilerplate","content","java"]},{"location":"cookbooks/content_basics/#properties","title":"Properties","text":"<p>The contents in Fast2 also embed properties, for more closely related data.</p> <pre><code>Collection&lt;Property&gt; myProps = myContent.getProperties();\nString myValue = myContent.getProperty(myName);\nmyContent.setProperty(myName,myValue);\n</code></pre>","tags":["boilerplate","content","java"]},{"location":"cookbooks/content_basics/#sub-contents","title":"Sub-contents","text":"<p>Subcontents are just regular contents stored into a ContentSet attached to a content.</p> <p>They can be both created/added and removed:</p> <pre><code>ContentSet subContents = myContent.getSubContents();\nmyContent.clearSubContents();\n</code></pre>","tags":["boilerplate","content","java"]},{"location":"cookbooks/data_from_filename/","title":"Add data from file name","text":"<p>Times will happen when you will not be able to rely on side-file metadata documents to map onto the documents you are migrating. The data will be concatenated into the file name.</p> <p>Fortunatelly with Fast2, there is still a possibility to parse this file name and pull out the required metadata. Last step would be to tight them down into the document dataset.</p>","tags":["worker","json","javascript","boilerplate"]},{"location":"cookbooks/data_from_filename/#where-do-we-come-from","title":"Where do we come from ?","text":"<p>For the educational aspect of this topic, let us consider a folder gathering several documents, all with the same format : <code>&lt;document-type&gt;-&lt;data1&gt;-&lt;data2&gt;</code>.</p> <p>Our folder looks like this:</p> <pre><code>\u3134 folder-to-extract\n    |\n    \u3134 contract-123-ABC.pdf\n    \u3134 contract-346-DEF.pdf\n    |\n    \u3134 bill-123-ABC.pdf\n    \u3134 bill-346-DEF.pdf\n    |\n    \u3134 draft-123-ABC.pdf\n    \u3134 draft-346-DEF.pdf\n</code></pre>","tags":["worker","json","javascript","boilerplate"]},{"location":"cookbooks/data_from_filename/#where-to-go","title":"Where to go ?","text":"<p>At a glance, we are just 3 (major) steps away from having a PDF content in our punnet, with a basic dataset populated from the JSON metadata :</p> <ol> <li>Scan the parent folder and list all the documents with names to map,</li> <li>Get the document path, and isolate the file name</li> <li>Parse the file name and attach the metadata to the dataset. For this example, data will be mapped onto the document dataset.</li> </ol> <p> </p>","tags":["worker","json","javascript","boilerplate"]},{"location":"cookbooks/data_from_filename/#way-to-go","title":"Way to go !","text":"<p>Inside Fast2, the map design is now pretty straightforward, given our ideas are rather clear in terms of the overall order of the operations.</p> <p>The map is even quite close to the 3 steps detailed earlier. The LocalSource task just needs to be given the path of the folders to deal with. This task will also identify the file name and attach the metadata to the document dataset.</p> <p>Then the JSTranform will retrieve the corresponding document path, and carry on with the data mapping.</p> <p>That way, we end up with 4 tasks :</p> <ul> <li>LocalSource, to collect the documents from local storage,</li> <li>JSTranform, whose role will be to :  parse the file name  add the data to the dataset</li> </ul> <p></p> <p></p> <p> </p>","tags":["worker","json","javascript","boilerplate"]},{"location":"cookbooks/data_from_filename/#javascript-elaboration","title":"JavaScript elaboration","text":"<p>Although the configuration of the first task can be easily guessed, the JSTranform final resulting script should look something like this :</p> <pre><code>punnet.getDocuments().forEach(function (doc) {\n// (1)\nvar filenameWithoutExtension = doc\n.getDataSet()\n.getData(\"fileName\")\n.getValue()\n.split(\".\")[0];\n// (2)\nvar data = filenameWithoutExtension.split(\"-\");\n// (3)\ndoc.getDataSet().addData(\"document-type\", \"String\", data[0]);\ndoc.getDataSet().addData(\"data1\", \"String\", data[1]);\ndoc.getDataSet().addData(\"data2\", \"String\", data[2]);\n});\n</code></pre> <ol> <li> Get the filename, and remove the extension</li> <li> Parse the filename with the separator character</li> <li> Attach the data</li> </ol> <p> </p> <p>Head out now to the Run screen, start your campaign and just... enjoy !</p>","tags":["worker","json","javascript","boilerplate"]},{"location":"cookbooks/data_from_filename/#result","title":"Result","text":"<p>At the latest stage of your workflow, the document dataset is filled with the properties found in the JSON and integrated as metadata.</p> <pre><code>{\n\"punnetId\": \"document-123-ABC.pdf#1\",\n\"documents\": [\n{\n\"documentId\": \"document-123-ABC.pdf\",\n\"data\": {\n\"absolutePath\": \"C:\\\\samples\\\\document-123-ABC.pdf\",\n\"fileName\": \"document-123-ABC.pdf\",\n\"absoluteParentPath\": \"C:\\\\samples\",\n\"length\": {\n\"value\": \"18700\"\n},\n\"lastModified\": {\n\"value\": \"Mon Dec 27 14:10:47 CET 2021\",\n\"type\": \"Date\"\n},\n\"document-type\": \"document\",\n\"data1\": \"123\",\n\"data2\": \"ABC\"\n},\n\"contents\": {\n\"url\": \"C:\\\\samples\\\\document-123-ABC.pdf\"\n},\n\"folders\": [...]\n}\n]\n}\n</code></pre>","tags":["worker","json","javascript","boilerplate"]},{"location":"cookbooks/data_from_filename/#lets-sum-up","title":"Let's sum up","text":"<p>We can bring this scenario further by mapping data from the parent folder(s). We would just need the document path, which can be retrieved easily, as explained in the advanced section of how to handle the JS Tranform task.</p> <p>For a OS-proofed script (Linux or Windows have their own subtleties when it comes to paths), you may need to make sure the parsing is done correctly, by standardizing the folder-architecture-related special characters from the Windows <code>\\</code> to a regular <code>/</code>.</p> <p>If this use-case echoes your early needs, other tasks can be tied to this map to reach a higher level of complexity characteristic of real-world migration projects.</p>","tags":["worker","json","javascript","boilerplate"]},{"location":"cookbooks/dataset_basics/","title":"Datasets","text":"<p>Datasets are Fast2 objects which can be involved at different levels within the punnet.</p> <p>They can be found on the</p> <ul> <li>punnets</li> <li>documents</li> <li>workflows</li> </ul> <p>Datasets gather data which can be manipulated to store properties, and can be accessed as follows:</p> <pre><code>DataSet dataset = punnet.getDataSet();\nDataSet dataset = document.getDataSet();\nDataSet dataset = workflow.getDataSet();\n</code></pre> <p>Since datasets are just groups of data, understanding basic operations with data is primordial.</p>","tags":["boilerplate","dataset","java","data"]},{"location":"cookbooks/dataset_basics/#data-object","title":"Data object","text":"<p>In Fast2, a data has 3 different informations:</p> <ul> <li>its name,</li> <li>its type (<code>String</code> or <code>int</code>)</li> <li>its value(s)</li> </ul> <p>The following line retrieve the data as object :</p> <pre><code>Data data = dataset.getData(dataName);\n</code></pre>","tags":["boilerplate","dataset","java","data"]},{"location":"cookbooks/dataset_basics/#name","title":"Name","text":"<p>Getting the name of a data just goes like:</p> <pre><code>String dataName = data.getSymbolicName();\n</code></pre>","tags":["boilerplate","dataset","java","data"]},{"location":"cookbooks/dataset_basics/#type","title":"Type","text":"<p>If no type has been defined when the data has been created, the data type will be <code>null</code>.</p> <p>However Fast2 will treat the value of the data as a regular <code>String</code>.</p> <pre><code>String dataType = data.getType();\n</code></pre>","tags":["boilerplate","dataset","java","data"]},{"location":"cookbooks/dataset_basics/#values","title":"Value(s)","text":"<p>When dealing with data, some can be single-valued while others can be multi-valued.</p> <p>The returned object will differ accordingly.</p> <pre><code>String dataValue = data.getValue();\nList&lt;String&gt; dataValues = data.getValues();\n</code></pre> <p>Data values can be added along the way, even when the data has already been created with a given value to begin with:</p> <pre><code>data.addValue(value);\n</code></pre> <p> </p>","tags":["boilerplate","dataset","java","data"]},{"location":"cookbooks/dataset_basics/#properties","title":"Properties","text":"<p>A data can be dealt with just like any other object with properties.</p> <p>Therefore, adding a property, removing it or getting it are just as simple as you would think:</p> <pre><code>data.setProperty(name, value);\nString value = data.getProperty(name);\ndata.removeProperty(name);\n</code></pre>","tags":["boilerplate","dataset","java","data"]},{"location":"cookbooks/dataset_basics/#add-data","title":"Add data","text":"<p>Several ways of adding data to the dataset are available, depending on the type of value you are willing to store:</p> <pre><code>Data myData = myDataset.addData(name, \"String\", value);  // String\nData myData = myDataset.addData(name, null, value);\nData myData = myDataset.addData(name, \"boolean\", true);  // boolean\nData myData = myDataset.addData(name, type, 10);         // long, int\nData myData = myDataset.addData(name, type);             // list or arrays of String\nmyData.addAll(Arrays.asList(\"a\", \"b\", \"c\"));\n</code></pre> <p>Adding a new data with the same name as an already stored one, will result in overwriting the existing value with the new one.</p>","tags":["boilerplate","dataset","java","data"]},{"location":"cookbooks/dataset_basics/#iterating-through-all-data","title":"Iterating through all data","text":"<p>Data mapping often requires to cover all data, no matter their name. To do so, the easiest way is to get them as a list:</p> <pre><code>List&lt;Data&gt; allData = myDataset.getData();\nfor(Data data : allData){\n// ...\n}\n</code></pre>","tags":["boilerplate","dataset","java","data"]},{"location":"cookbooks/dataset_basics/#retrieve-data-values","title":"Retrieve data value(s)","text":"<p>The following line retrieve the data as object :</p> <pre><code>Data data = dataset.getData(dataName);\n</code></pre>","tags":["boilerplate","dataset","java","data"]},{"location":"cookbooks/dataset_basics/#single-valued-data","title":"Single-valued data","text":"<p>The dataset offers a shortcut to get the value(s) of any data:</p> <pre><code>// 1st way : via data object\nString value = myDataSet.getData(dataName).getValue();\n// or\n// 2nd way : dataset shortcut\nString value = myDataSet.getDataValue(dataName);\n</code></pre> <pre><code>public String getDataValue(String name);\n</code></pre>","tags":["boilerplate","dataset","java","data"]},{"location":"cookbooks/dataset_basics/#multi-valued-data","title":"Multi-valued data","text":"<pre><code>// 1st way : via data object\nList&lt;String&gt; value = myDataSet.getData(dataName).getValues();\n// or\n// 2nd way : dataset shortcut\nList&lt;String&gt; value = myDataSet.getDataValues(dataName);\n</code></pre>","tags":["boilerplate","dataset","java","data"]},{"location":"cookbooks/dataset_basics/#remove-data","title":"Remove data","text":"<p>If the data has been found and could successfully be removed, the following method will return <code>TRUE</code>:</p> <pre><code>boolean removeSuccessful = myDataset.removeData(name);\n</code></pre>","tags":["boilerplate","dataset","java","data"]},{"location":"cookbooks/dataset_basics/#check-if-data-exists","title":"Check if data exists","text":"<p>Rely on this method to make sure not to overwrite any existing data, nor having a <code>DataNotFoundException</code> exception.</p> <pre><code>boolean exists = myDataset.hasData(name);\n</code></pre>","tags":["boilerplate","dataset","java","data"]},{"location":"cookbooks/dataset_basics/#datanotfound-exception","title":"DataNotFound exception","text":"<p>When operations are performed on non-existing data, an exception of type <code>DataNotFoundException</code> is thrown.</p>","tags":["boilerplate","dataset","java","data"]},{"location":"cookbooks/document_basics/","title":"Documents management","text":"<p>The documents are a main part of any migration, if not the purpose of it. Here are the basics for an appropriate understanding of the Java API of the document object.</p> <p>For a better understanding of the following, the document structure needs to be clearly visualized. If required, go back to the definition of such an object in Fast2.</p>","tags":["boilerplate","document","dataset","content","java"]},{"location":"cookbooks/document_basics/#creation","title":"Creation","text":"<p>As explained in the basics of punnet a document can be created on the fly:</p> <pre><code>Document myDoc = myPunnet.addDocument(DocumentId.id());\n</code></pre> <p>If required, the document ID can be force by adding a parameter into the document ID creation:</p> <pre><code>Document myDoc = myPunnet.addDocument(DocumentId.id(\"myDocId\"));\n</code></pre> <p>However the ID can be forced after the document creation:</p> <pre><code>myDoc.setDocumentId(DocumentId.id(\"myDocId\"));\n</code></pre> <p>Later on, its ID can be retrieved just like the punnet's:</p> <pre><code>DocumentId myId = myDoc.getDocumentId();\n</code></pre>","tags":["boilerplate","document","dataset","content","java"]},{"location":"cookbooks/document_basics/#dataset","title":"DataSet","text":"<p>The purpose of the document dataset is to store metadata closely related to its entity. When data are not too tighly related to the content of a document, chances are they will be stored as this dataset level. The mime-type does not follow this rule, though.</p> <p>This dataset can be access via an usual getter:</p> <pre><code>DataSet myDataset = myDoc.getDataSet();\n</code></pre> <p>A document is built with an empty dataset by default.</p> <p>For more information concerning the dataset, head out to the dedicated section.</p>","tags":["boilerplate","document","dataset","content","java"]},{"location":"cookbooks/document_basics/#contents","title":"Contents","text":"<p>In Fast2, a document can have no to several contents.</p> <p>No-content cases could be like:</p> <ul> <li>the document does not have a content, originally</li> <li>the content has already been migrated</li> <li>the migration is just an update with only a few metadata to send to the destination</li> </ul> <p></p> <p>Several-contents cases could be like:</p> <ul> <li>the document has attachments (1 content per attachment)</li> <li>the document has content of different types (e.g. a PDF file alongside a TIFF file)</li> </ul> <p>However the ratio 1-content-for-1-document is quite common.</p> <p>Contents are accessed via the <code>ContentSet</code> which basically is a collection of <code>ContentContainer</code>s:</p> <pre><code>ContentSet myContents = myDoc.getContentSet();\n</code></pre> <p>A document is built with an empty contentset by default.</p> <p>For more information concerning the contentset, head out to the dedicated section.</p>","tags":["boilerplate","document","dataset","content","java"]},{"location":"cookbooks/document_basics/#mime-type","title":"Mime-type","text":"<p>All documents provide a shortcut to their first content mime-type, under the <code>mimeType</code> data stored in the document dataset:</p> <pre><code>String myMimetype = myDoc.getMimeType();\n</code></pre> <p>This data can also be set from the document level:</p> <pre><code>myDoc.setMimeType(\"myMimetype\");\n</code></pre> <p>As said earlier, this method is just a shortcut to add a mime-type data into the document dataset.</p>","tags":["boilerplate","document","dataset","content","java"]},{"location":"cookbooks/document_basics/#folders","title":"Folders","text":"<p>The document folderset can be used for folders-only migration and well as folder-as-a-whole ones.</p> <p>A folderset can contains one or several folder references, and is access as follows:</p> <pre><code>FolderSet myFolders = myDoc.getFolders();\n</code></pre> <p>A document is built with an empty folderset by default.</p> <p>For more information concerning the folders, head out to the dedicated section.</p>","tags":["boilerplate","document","dataset","content","java"]},{"location":"cookbooks/document_basics/#annotations","title":"Annotations","text":"<p>The document can embed zero to several annotations alongside its contents or data.</p> <p>The collection of these annotations is called an <code>AnnotationSet</code>, and can be accessed as follows:</p> <pre><code>AnnotationSet myAnnotationSet = myDoc.getAnnotationSet();\nList&lt;Annotation&gt; myAnnotions = myAnnotationSet.getAnnotationList();\nmyAnnotationSet.addAnnotation(myAnnotation);\nAnnotation newAnnotation =  myAnnotationSet.addAnnotation(myAnnotatioContent);\n</code></pre> <p>A document is built with an empty annotationset by default.</p> <p>From a Fast2 standpoint, a annotation is just a object composed by an ID and a content.</p>","tags":["boilerplate","document","dataset","content","java"]},{"location":"cookbooks/punnet_basics/","title":"Punnets management","text":"<p>Since the punnet is the pivot format into Fast2, all documents, folders and metadata can only be manipulated through this object. Here are the basics for an appropriate understanding of the Java API of the punnet object.</p> <p>For a better understanding of the following, the punnet structure needs to be clearly visualized. If required, go back to the definition of such an object in Fast2.</p>","tags":["boilerplate","punnet","content","java"]},{"location":"cookbooks/punnet_basics/#creating-a-punnet","title":"Creating a punnet","text":"<p>The creation of a punnet is an operation which should only take place in source tasks, as all other tasks only process punnets given to the as input.</p> <p>To create a punnet:</p> <pre><code>Punnet myPunnet = task.getManager().getPunnetFactory().createEmptyPunnet();\n</code></pre> <p>The punnet ID will be automatically computed by Fast2, following certain models based on the task producing the punnet. This ID can be useful to track the punnet over different tasks, as its value will remain unchanged.</p>","tags":["boilerplate","punnet","content","java"]},{"location":"cookbooks/punnet_basics/#id","title":"ID","text":"<p>To force the ID of the punnet, a <code>String</code> value can be passed as argument.</p> <pre><code>myPunnet.setPunnetId(PunnetId.id(\"myId\"));\n</code></pre> <p>Retrieving the ID of the punnet just goes as so:</p> <pre><code>PunnetId punnetId = myPunnet.getPunnetId();\n</code></pre>","tags":["boilerplate","punnet","content","java"]},{"location":"cookbooks/punnet_basics/#sending-a-punnet-into-the-campaign","title":"Sending a punnet into the campaign","text":"<p>The source tasks are responsible for not only create the punnet, but also send it in the migration workflow.</p> <p>Once the punnet is created and its components are correctly formed, the last step of the source will rely on the <code>Consumer</code> input parameter of the main method of the connector :</p> <pre><code>consumer.push(myPunnet);\n</code></pre>","tags":["boilerplate","punnet","content","java"]},{"location":"cookbooks/punnet_basics/#documents","title":"Documents","text":"<p>Punnets may or may not embed document(s).</p> <p>Several-documents cases could be like:</p> <ul> <li>the migration strategy is to group in a punnet all documents stored in a given folder</li> <li>the migration strategy is to group in a punnet all documents matching a criterion (depending of the configuration fields of the source)</li> <li>a punnet embed a released version of a document and all its past versions.</li> </ul> <p>Whatever the reason, the documents can be added to the punnet in two ways:</p> <ol><li> The document was already existing:  <pre><code>Document myDoc = myPunnet.addDocument(myDocument);\n</code></pre> </li><li> The document needs to be created, which can be done on the fly: <pre><code>Document myDoc = myPunnet.addDocument(DocumentId.id());\n</code></pre> </li></ol> <p>All documents can be access via the list of all documents stored in the punnet:</p> <pre><code>List&lt;Document&gt; myDocuments = myPunnet.getDocuments();\n</code></pre> <p>As any Java list, documents can be removed as long as the correct index is provided.</p> <p>For more information concerning the documents, head out to the dedicated section.</p>","tags":["boilerplate","punnet","content","java"]},{"location":"cookbooks/punnet_basics/#dataset","title":"DataSet","text":"<p>The purpose of the punnet dataset is to store metadata not closely related to any folder or document specifically.</p> <p>This dataset can be access via an usual getter:</p> <pre><code>DataSet myDataset = myPunnet.getDataSet();\n</code></pre> <p>A punnet is built with an empty dataset by default.</p> <p>For more information concerning the dataset, head out to the dedicated section.</p>","tags":["boilerplate","punnet","content","java"]},{"location":"cookbooks/punnet_basics/#folders","title":"Folders","text":"<p>The punnet folderset can be used for folders-only migration and well as folder-as-a-whole ones, where a punnet will contain a folder reference and all the documents previously filed into this folder.</p> <p>A folderset can contains one or several folder references, and is access as follows:</p> <pre><code>FolderSet myFolders = myPunnet.getFolders();\n</code></pre> <p>A punnet is built with an empty folderset by default.</p> <p>For more information concerning the folders, head out to the dedicated section.</p>","tags":["boilerplate","punnet","content","java"]},{"location":"getting-started/","title":"Is this page still useful ??","text":""},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<p>The installation of Fast2 requires a few environment specifications to run properly :</p> What Description RAM 8GB+ We highly recommend having at least 8GB. When switching to production environments, 16GB or 32GB will be required since more documents will be handled at once, and heavy tasks (e.g. conversion, extraction) might get short on resources. Processor 8 CPUs Processor capabilities need to be aligned with migration requirements, such as data mapping, content conversion and heavy I/O. Storage 128GB+ Although the contents dealt by Fast2 will be temporarily stored (and deleted afterwards if asked), the server needs enough storage for the files/contents alongside the database tracking all the migration information. Java JRE8, JRE11 Any provider will fit (Oracle, OpenJDK, etc). If you have multiple JDK/JRE already installed, specify the correct one in the <code>./config/env.properties</code> file. OS Windows\u00a07+, Linux All versions of Windows 7+ are supported. All common distros of Linux are supported (Ubunto, RedHat, CentOS, etc)Power architecture are supported as well (except the ones running in AIX), but only Java parts will work seamlessly whereas third-party software (e.g. imagemagick, libreoffice, etc) might not, as they have not all have been developed for such plateforms. <p></p> <p>While setting up the production server for Fast2, make sure to scale the Fast2 machine accordingly. You may need to increase the allocated memory for both the broker and the background database. If you planned to deal with campaigns of a few millions of documents, setting 8GB of memory for the broker and 8GB for the database as well is a good starting point.</p> <p>Warning</p> <p>If you decide to go for a custom Elasticsearch database, make sure to confirm the compatibility with your environment at Elasticsearch Support Matrix.</p>"},{"location":"getting-started/installation/#fast2-packages","title":"Fast2 packages","text":"<p>The Fast2 distribution you need depends on your target environment. It exists three way to deploy a Fast2 :</p> <p>  On premise: regular package, as an all-in-one zip file  AWS: Standard AMIs  K8S: Docker Images</p> <p>Each distribution ships the following</p> <ul> <li>A broker with one embedded worker and a user interface</li> <li>An additional worker with all tasks catalog</li> <li>A template to create workers with custom tasks</li> </ul>"},{"location":"getting-started/installation/#root-folder-anatomy","title":"Root folder anatomy","text":"Item Purpose  config Configuration files, broker endpoint, Java home  logs Logging files for both broker and worker(s)  maps XML files of all maps accessible from the UI  database Either Elasticsearch or OpenSearch  service All files required to start Fast2 as a service  worker-libs/* All libraries and dependencies for tasks executions  fast2-broker-package-X.Y.Z.jar Broker unit  fast2-worker-package-X.Y.Z.jar Worker main unit  startup-broker.bat Binary file for Windows  startup-broker.sh Binary file for Linux  startup-worker.bat Binary file for Windows  startup-worker.sh Binary file for Linux"},{"location":"getting-started/installation/#start-up-sequence","title":"Start-up sequence","text":"<p>When Fast2 is started, either as a standalone application or a service, its different modules share a precisely defined roll-out schedule:</p> <ul> <li>First, the broker and its internal databse are started. The connection between these 2 components has to be effective, otherwise Fast2 will automatically shut down after a couple of attempts to reach the database;</li> <li>The worker is then triggered, and has to register itself to the broker.</li> <li>Finally, the dashboard will be started if asked so, and if the binaries have been detected. First, Fast2 will try to connect to any dashboard instance running on the configured port.</li> </ul> <p>There is no direct connection between the broker and the dashboard. The only exchange area is the Elasticsearch database, as explained in the architecture section.</p>"},{"location":"getting-started/installation/#start-fast2-broker","title":"Start Fast2 Broker","text":"<p>Once the regular Fast2 package is unzipped, Fast2 can be launched right away.</p> <p>Whether Fast2 is launched from the batch file or as a service on your environment, the UI will be available at http://localhost:1789/.</p> <p>By default, Fast2 Broker starts an embedded Elastic Search and an embedded Fast2 Worker.</p> <p>All commands below are to be run under the Fast2 install path (where the Zip has been unzipped).</p>"},{"location":"getting-started/installation/#from-command-line","title":"From command line","text":"Windows Linux <p>Go into the Fast2 install folder, and run :</p> <pre><code>C:\\path-to-fast2\\&gt; startup-broker.bat\n</code></pre> <p>Administrator rights might be required since Fast2 will handle some port communications.</p> <p>The following Linux installation steps work for most of Unix-based sytems. Elasticsearch cannot be started from root user, you will need to create a secondary user to start the database binary alongside your Fast2 process.</p> <p>Once connected as the latter user, start Elasticsearch via its binaries.</p> <p>Then, since you started Elasticsearch manually, disable the command triggering Fast2 to start the embedded Elasticsearch, from the <code>config/application.properties</code> file :</p> <pre><code>broker.elasticsearch.embedded.enabled=false\n</code></pre> <p>You can now properly execute the following script:</p> <pre><code>$ ./startup-broker.sh\n</code></pre> <p>To end the Fast2 process, just hit <code>Ctrl+C</code> in the command line the startup file opened.</p>"},{"location":"getting-started/installation/#as-service","title":"As service","text":"Windows Linux <p>Go into the Fast2 installation folder, and open the Windows Command Prompt.</p> <p>To install the service :</p> <pre><code>C:\\path-to-fast2\\service\\windows&gt; Fast2_broker_service.exe install\n</code></pre> <p>Your machine may prompt a message asking to download .NET components. Please click [OK] and proceed.</p> <p>Once this command is complete, you should see in your services registory a newly installed Fast2 service. You can start/stop/restart it as any other service, or via the Command Prompt (just replace <code>install</code> in the previous command by start/stop/uninstall/restart/status).</p> <p>The logs of the service will be available from the <code>path-to-fast2\\service\\log</code> folder.</p> <p>There are several ways to create a service under linux distribution. We will do it through systemd. Its major benefit is that it has been the default init system for the majority of linux distributions (Ubuntu, Red Hat, Fedora...).</p> <p></p>"},{"location":"getting-started/installation/#create-a-user-for-fast2","title":"Create a user for Fast2","text":"<p>Since the embedded database cannot be started in sudo mode, an additional user needs to be created in the Linux machine so the broker will successfully initiate the database bootup.</p> <p>Let's condider here our user to be fast2user.</p> <p>Head out to the <code>./startup-broker.sh</code> and get the user start the broker by switching users (with <code>su fast2user -c</code>) for the Java command.</p> <p>The result should be as follows:</p> <pre><code>- \"$JAVA\" -Xmx$BROKER_MAX_MEMORY -jar fast2-broker-package-X.Y.Z.jar\n+ su fast2user -c \"$JAVA -Xmx$BROKER_MAX_MEMORY -jar fast2-broker-package-X.Y.Z.jar\"\n</code></pre> <p></p>"},{"location":"getting-started/installation/#execution-path","title":"Execution path","text":"<p>Edit the <code>ExectStart</code> field from the file <code>service/linux/fast2-broker.service</code> by changing the <code>PATH/TO/FAST2</code> portion: set it to Fast2 install path.</p> <pre><code>[Unit]\nDescription=Fast2 Broker\n\n[Service]\nExecStart=/home/userName/fast2-complete-package-2.0.0/startup-broker.sh\n\n[Install]\nWantedBy=default.target\n</code></pre> <p></p>"},{"location":"getting-started/installation/#symbolic-link","title":"Symbolic link","text":"<p>Now link it to the <code>/etc/systemd/system</code> directory through a symbolic link.</p> <pre><code>$ sudo ln -s SOURCE TARGET\n\n$ sudo ln -s service/linux/fast2-broker.service /etc/systemd/system\n</code></pre> <p>If the links are broken once they're created, you probably need to put an absolute path for the target as follow ;</p> <pre><code>$ sudo ln -s /home/userName/fast2-complete-package-2.0.0/service/linux/fast2-broker.service /etc/systemd/system\n</code></pre> <p>Next, reload systemd services unit and enable them :</p> <pre><code>$ systemctl daemon-reload\n$ systemctl enable fast2-broker.service\n</code></pre> <p>The terminal should prompt the following message :</p> <pre><code>Created symlink from /etc/systemd/system/default.target.wants/fast2-broker.service to /etc/systemd/system/fast2-broker.service.\n</code></pre> <p></p>"},{"location":"getting-started/installation/#script-uses","title":"Script uses","text":"<p>Test your script by starting it and then checking the status :</p> <pre><code>$ service fast2-broker start\n$ service fast2-broker status\n</code></pre> <p>or</p> <pre><code>$ systemctl start fast2-broker.service\n$ systemctl status fast2-broker.service\n</code></pre> <p>You can restart or stop the service at anytime with the commands :</p> <pre><code>$ service fast2 start | restart | stop | status\n</code></pre>"},{"location":"getting-started/installation/#start-fast2-worker","title":"Start Fast2 Worker","text":"<p>The Broker starts an embedded worker by default.</p>  Windows Linux <p>If you wish to start multiple workers, just hit :</p> <pre><code>C:\\path-to-fast2\\&gt; startup-worker.bat\n</code></pre> <pre><code>./startup-worker.sh\n</code></pre> <p>If the worker and broker are not booted up on the same machine, you need to setup the Broker host name in the worker configuration file. Edit the file <code>config/application.properties</code> and modify <code>broker.host</code> accordingly.</p> <p>You can setup Fast2 Worker as a service the same way you did for the Fast2 Broker.</p>"},{"location":"getting-started/overall-concepts/","title":"What you need to know before committing to Fast2","text":""},{"location":"getting-started/overall-concepts/#basic-jargon","title":"Basic jargon","text":"Source <p>A source is a Fast2 task whose role is to gather the documents or items to migrate. As they are identified, the source converts them into punnets.</p> Punnet <p>The Punnet is the pivot format which is used for data mapping, content conversions and folder management. This is the migration entity, processed and then forwarded by the workflow tasks.</p> Task <p>A task is either an extract-, transform- or injection-step that composes a workflow. Each task can be configured to match the user\u2019s needs. Once all tasks are completed in the specific order, the migration is over.</p> Map <p>A workflow (aka \"Map\") is a succession of tasks, where the output of the ones is the input of the following others. Each task can be considered as a step of the workflow.</p> Campaign <p>A campaign is the perimeter where a map is executed (once or several times). Different campaigns can either be cumulative or independent.</p> Worker <p>The Worker is the punnet processor, applying the changes onto the punnet, according to how the tasks have been configured by the user.</p> <p>They are waiting in silence to do their job. When a punnet needs to be processed by a task, the broker triggers the assigned worker.</p> <p>If the workload is too important, you can manually add workers to speed up processing.</p> Broker <p>The broker is the trump card of the migration. It is basically the workflow orchestrator, in charge of database communication, sending punnets to the worker(s) for them to process the operations.</p> <p>Scheduling, orchestrating or even managing queues : the broker is everywhere.</p> <p>His first job is to handle the workers. Worker coordination is a key point in terms of performance, knowing that there may be a multitude of them.</p> <p>In addition, the broker ensures the persistence and traceability of the data carried out by the punnets into the database, where logs, data and errors and more are stored.</p> <p> </p>"},{"location":"getting-started/overall-concepts/#architecture","title":"Architecture","text":""},{"location":"getting-started/overall-concepts/#fast2-objects","title":"Fast2 objects","text":""},{"location":"getting-started/overall-concepts/#folder","title":"Folder","text":"<p>Folder object represents a folder in the ECM or file system sense, and can have metadata as well as links to documents</p> <p>Info</p> <p>This type can be included into punnets and documents, and folders themselves.</p> <pre><code>\u3134 folder\n    \u3134 name\n    \u3134 path\n    \u3134 parent folder\n        \u3134 name\n        \u3134 path\n        \u3134 parent folder\n            \u3134 ...\n</code></pre>"},{"location":"getting-started/overall-concepts/#dataset","title":"Dataset","text":"<p>Data object represents metadata in the ECM sense. It contains a name, a type, and one or more values.</p> <p>Info</p> <p>This type can be included into punnets, documents and workflows.</p> <pre><code>\u3134 dataset\n    \u3134 metadata A (ex/ key: value)\n        \u3134 properties\n            \u3134 property\n            \u3134 property\n            \u3134 ...\n    \u3134 metadata B (ex/ key: [value A, value B])\n    \u3134 ...\n</code></pre>"},{"location":"getting-started/overall-concepts/#content","title":"Content (aka 'ContentContainer')","text":"<p>Content object materializes document content that can be simple or made up of several pages. It can be materialized by a relative or absolute path to its storage location or stored directly in memory / in an XML file.</p> <p>Info</p> <p>This type can be included into documents and annotations.</p> <pre><code>\u3134 content\n    \u3134 URL\n    \u3134 mime-type\n    \u3134 properties\n        \u3134 property\n        \u3134 property\n        \u3134 ...\n    \u3134 subcontents\n        \u3134 content\n        \u3134 content\n        \u3134 ...\n</code></pre>"},{"location":"getting-started/overall-concepts/#annotations","title":"Annotations","text":"<p>Annotation object represents an annotation (post-its, arrow\u2026) affixed to the content of a document. This object is not conceptualized in all ECM systems.</p> <pre><code>\u3134 annotation\n    \u3134 ID\n    \u3134 content\n</code></pre>"},{"location":"getting-started/overall-concepts/#document","title":"Document","text":"<p>A document can also contains its own data, its content with annotations and the folder where it is stored.</p> <p>Info</p> <p>This type can be included into punnets and workflows.</p> <pre><code>\u3134 document\n    \u3134 ID\n    \u3134 dataset\n    \u3134 contents\n    \u3134 mime-type\n    \u3134 folders\n    \u3134 annotations\n</code></pre>"},{"location":"getting-started/overall-concepts/#workflow","title":"Workflow","text":"<pre><code>\u3134 workflows\n    \u3134 dataset\n    \u3134 associated documents\n        \u3134 document\n        \u3134 document\n        \u3134 ...\n</code></pre>"},{"location":"getting-started/overall-concepts/#punnet","title":"Punnet","text":"<p>As introduced above, the punnet gathers all the different assets to migrate.</p> <pre><code>\u3134 punnet\n    \u3134 ID\n    \u3134 documents\n    \u3134 dataset\n    \u3134 workflows\n    \u3134 folders\n</code></pre> <p>When serialized in XML format, it will look roughly like :</p> <pre><code>&lt;?xml version='1.0' encoding='UTF-8'?&gt;\n&lt;ns:punnet xmlns:ns=\"http://www.arondor.com/xml/document\" punnetId=\"34c5434c-4234-4fa2-9f91-7882a899a994#1\"&gt;\n&lt;ns:documentset&gt;\n&lt;ns:document documentId=\"34c5434c-4234-4fa2-9f91-7882a899a994\"&gt;\n&lt;ns:contentset&gt;\n&lt;com.arondor.fast2p8.model.punnet.ContentContainer contentStorage=\"URL\"&gt;\n&lt;ns:url&gt;C:/samples/file.pdf&lt;/ns:url&gt;\n&lt;/com.arondor.fast2p8.model.punnet.ContentContainer&gt;\n&lt;/ns:contentset&gt;\n&lt;ns:dataset&gt;\n&lt;ns:data name=\"name\" type=\"String\"&gt;\n&lt;ns:value&gt;sample&lt;/ns:value&gt;\n&lt;/ns:data&gt;\n&lt;/ns:dataset&gt;\n&lt;ns:folderset&gt;\n&lt;ns:folder parent-path=\"/primary-folder/subfolder\" name=\"sample\"&gt;\n&lt;ns:dataset /&gt;\n&lt;/ns:folder&gt;\n&lt;/ns:folderset&gt;\n&lt;ns:annotationset /&gt;\n&lt;/ns:document&gt;\n&lt;/ns:documentset&gt;\n&lt;ns:dataset /&gt;\n&lt;folderSet /&gt;\n&lt;/ns:punnet&gt;\n</code></pre>"},{"location":"getting-started/overall-concepts/#lifecycle","title":"Lifecycle","text":"<p>The punnet will iterate through the follwing lifecycle until the last step is reached.</p> <pre><code>graph TD\n    A(Created) --&gt; B(Queued);\n    B --&gt; C(Processing);\n    C --&gt; D{ };\n    D --&gt;|Success| F(Processed OK);\n    D --&gt;|Failure| E(Processed KO);\n    F -. Next task ? .-&gt; B;\n    E -.-&gt; |Retry| H( );\n    F -.-&gt; |Retry| H;\n    H -.-&gt; I( );\n    I -.-&gt;|Previous try| J{ };\n    I -.-&gt; B;\n    J -.-&gt;|KO| L[SupersededException];\n    J -.-&gt;|OK| K[SupersededOK];</code></pre>"},{"location":"getting-started/overall-concepts/#task","title":"Task","text":"<p>Task can be represented as a processing unit to be applied to a punnet. A punnet comes at the entry of the task, as an input. The task performs operations and then outputs the modified punnet.</p> <p>During the processing of each task, statistics are collected allowing to know the number of punnets processed per second. This is the actual throughput of the task and it is of course dependent on the environment (neighboring tasks, multi-thread\u2026) From this speed, Fast2 tries to estimate the average time left for all the tasks.</p> <p>One of the benefits of these statistics is the ability to visualize bottlenecks. Sometimes some tasks have a longer processing time than others. Thanks to the visualization of the queues, it is quite easy to know which task is greedy.</p> <p>When multiple tasks are linked together it represents a processing chain or a workflow where each punnet will be processed task by task. We will call this object a campaign.</p>"},{"location":"getting-started/overall-concepts/#map","title":"Map \u2015 workflow","text":"<p>// todo</p>"},{"location":"getting-started/overall-concepts/#campaign","title":"Campaign \u2015 workflow instance","text":"<p>As we have just seen, a campaign is made up of several tasks. In other words, it represents an instance of a map.</p> <p>Warning</p> <p>A campaign has a unique name</p> <p>Despite this uniqueness, each campaign can be ran multiple times. The statistical data of the new run will be added to the previous run(s).</p> <p>Other runs can be added on top of this one.</p> <p>A user has the opportunity to stop any campaign when he wants. He can always resume the campaign later, or start a fresh one.</p> <p>A retry feature is also available after each campaign. This makes possible to filter certain punnets and to replay them directly in the campaign. For instance, retry each punnet in exception. You can even select which type of exception you want.</p>"},{"location":"getting-started/overall-concepts/#lifecycle_1","title":"Lifecycle","text":"<pre><code>graph LR\n  A(Undefined) --&gt; B(Starting);\n  B --&gt; C(Started);\n  C --&gt; D(Running);\n  D -.-&gt;|Stops source iterator| F(Stopping);\n  D --&gt; E(Finished);\n  F -.-&gt;|Restart source iterator| B;</code></pre>"},{"location":"getting-started/overall-concepts/#operating","title":"Operating","text":""},{"location":"tags/","title":"Tags","text":"<p>Following is a list of relevant tags:</p>"},{"location":"tags/#drools","title":"Drools","text":"<ul> <li>Drools</li> </ul>"},{"location":"tags/#excel","title":"Excel","text":"<ul> <li>Drools</li> </ul>"},{"location":"tags/#java","title":"Java","text":"<ul> <li>Drools</li> <li>Patterns</li> </ul>"},{"location":"tags/#boilerplate","title":"boilerplate","text":"<ul> <li>Contents in Fast2</li> <li>Add data from file name</li> <li>Datasets</li> <li>Documents management</li> <li>Punnets management</li> </ul>"},{"location":"tags/#configuration","title":"configuration","text":"<ul> <li>Patterns</li> </ul>"},{"location":"tags/#content","title":"content","text":"<ul> <li>Contents in Fast2</li> <li>Documents management</li> <li>Punnets management</li> </ul>"},{"location":"tags/#data","title":"data","text":"<ul> <li>Datasets</li> </ul>"},{"location":"tags/#dataset","title":"dataset","text":"<ul> <li>Datasets</li> <li>Documents management</li> </ul>"},{"location":"tags/#document","title":"document","text":"<ul> <li>Documents management</li> </ul>"},{"location":"tags/#java_1","title":"java","text":"<ul> <li>Contents in Fast2</li> <li>Datasets</li> <li>Documents management</li> <li>Punnets management</li> </ul>"},{"location":"tags/#javascript","title":"javascript","text":"<ul> <li>Add data from file name</li> </ul>"},{"location":"tags/#json","title":"json","text":"<ul> <li>Add data from file name</li> </ul>"},{"location":"tags/#punnet","title":"punnet","text":"<ul> <li>Punnets management</li> </ul>"},{"location":"tags/#worker","title":"worker","text":"<ul> <li>Add data from file name</li> </ul>"}]}